{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "CABOCHA_ANALYZED_PATH = '../data/ai.ja/ai.ja.txt.cabocha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT_PATH = '../data/ai.ja/ai.ja.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cabocha をする前に、1文につき1行のフォーマットにしておく\n",
    "with open(INPUT_TEXT_PATH, \"r\") as fin:\n",
    "    texts = []\n",
    "    for line in fin:\n",
    "        splitted_lines = line.split(\"。\")\n",
    "        if len(splitted_lines) == 1:\n",
    "            texts += splitted_lines\n",
    "        else:\n",
    "            for splitted_line in splitted_lines:\n",
    "                if splitted_line != \"\\n\":\n",
    "                    texts.append(splitted_line + \"。\\n\")\n",
    "                else:\n",
    "                    texts.append(splitted_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人工知能\\n',\n",
       " '\\n',\n",
       " '人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。\\n',\n",
       " '「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/ai.ja/preprocessed_ai.ja.txt\", \"w\") as fout:\n",
    "    fout.write(\"\".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cabocha -f1 '../data/ai.ja/preprocessed_ai.ja.txt' >| '../data/ai.ja/ai.ja.txt.cabocha'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40\n",
    "\n",
    "from typing import List\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def is_sahen_noun(self):\n",
    "        return (self.pos == \"名詞\" and self.pos1 == \"サ変接続\")\n",
    "    \n",
    "    def is_wo(self):\n",
    "        return (self.base == \"を\" and self.pos1 == \"格助詞\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"surface[{self.surface}]base[{self.base}]pos[{self.pos}]pos1[{self.pos1}]\"\n",
    "    \n",
    "class Chunk:\n",
    "    def __init__(self, morphs, index, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.index = index\n",
    "        self.dst = dst\n",
    "        if srcs is None:\n",
    "            self.srcs = []\n",
    "        else:\n",
    "            self.srcs = srcs\n",
    "        if not isinstance(self.srcs, Iterable):\n",
    "            raise TypeError(f\"srcs must be iterable. {self.srcs}\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        sentence = \"\"\n",
    "        for snt in self.morphs:\n",
    "            sentence += snt.surface\n",
    "        return f\"{self.index}:{self.dst}:{self.srcs}:{sentence}\"\n",
    "                \n",
    "    @property\n",
    "    def surfaces(self):\n",
    "        sentence = \"\"\n",
    "        for snt in self.morphs:\n",
    "            sentence += snt.surface\n",
    "        if sentence[-1] in [\"。\", \"、\"]:\n",
    "            sentence = sentence[:-1]\n",
    "        return sentence\n",
    "    \n",
    "    def is_has_noun(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"名詞\":\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def is_has_verb(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"動詞\":\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def find_verb(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"動詞\":\n",
    "                return morph.base\n",
    "        return False\n",
    "    \n",
    "    def find_joshi(self):\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == \"助詞\":\n",
    "                return morph\n",
    "        return None\n",
    "    \n",
    "    def is_sahen_noun_and_wo(self):\n",
    "        has_sahen_noun = False\n",
    "        has_wo = False\n",
    "        for morph in self.morphs:\n",
    "            if morph.is_sahen_noun():\n",
    "                has_sahen_noun = True\n",
    "            if morph.is_wo():\n",
    "                has_wo = True\n",
    "            \n",
    "        return has_sahen_noun and has_wo\n",
    "    \n",
    "class Chunks():\n",
    "    def __init__(self, chunks: List):\n",
    "        self.chunks = chunks\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if chunk.dst >= len(self.chunks):\n",
    "                chunk.dst = -1\n",
    "            if chunk.dst != -1:\n",
    "                self.chunks[chunk.dst].srcs.append(i)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.chunks[index]\n",
    "        \n",
    "\n",
    "# 41\n",
    "class Sentence():\n",
    "    def __init__(self, chunks: Chunks):\n",
    "        self.chunks = chunks\n",
    "                \n",
    "    def __str__(self):\n",
    "        ans = \"\"\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            ans += str(chunk) + \"\\n\"\n",
    "        return ans\n",
    "    \n",
    "    def find_kaku_pattern(self):\n",
    "        verb = None\n",
    "        joshis = []\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if chunk.is_has_verb() == True:\n",
    "                verb = chunk\n",
    "                for src_index in chunk.srcs:\n",
    "                    src_chunk = self.chunks[src_index]\n",
    "                    morph = src_chunk.find_joshi()\n",
    "                    if morph is not None:\n",
    "                        joshis.append((morph.base, src_chunk.surfaces))\n",
    "                break\n",
    "                    \n",
    "        if verb is None:\n",
    "            return None\n",
    "        joshis = sorted(joshis, key=lambda x: x[0])\n",
    "        joshi_chars = [c[0] for c in joshis]\n",
    "        joshi_chunks = [c[1] for c in joshis]\n",
    "        joshis_str = \" \".join(joshi_chars)\n",
    "        joshis_chunks_str = \" \".join(joshi_chunks)\n",
    "        ret_str = verb.find_verb()\n",
    "        if len(joshis_str) > 0:\n",
    "            ret_str += f\" {joshis_str} {joshis_chunks_str}\"\n",
    "                    \n",
    "        return ret_str\n",
    "    \n",
    "    def find_sahen_kaku_pattern(self):\n",
    "        verb = None\n",
    "        sahen_noun = None\n",
    "        joshis = []\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if chunk.is_has_verb():\n",
    "                verb = chunk\n",
    "                for src_index in verb.srcs:\n",
    "                    # find nearest sahen wo kaku\n",
    "                    src_chunk = self.chunks[src_index]\n",
    "                    morph = src_chunk.find_joshi()\n",
    "                    if src_chunk.is_sahen_noun_and_wo():\n",
    "                        sahen_noun = src_chunk\n",
    "                        \n",
    "                if verb and sahen_noun:\n",
    "                    for src_index in verb.srcs:\n",
    "                        # build joshi list\n",
    "                        src_chunk = self.chunks[src_index]\n",
    "                        morph = src_chunk.find_joshi()\n",
    "                        if morph is not None and not src_chunk == sahen_noun:\n",
    "                            joshis.append((morph.base, src_chunk.surfaces))\n",
    "                    break\n",
    "                else:\n",
    "                    verb = None\n",
    "                    sahen_noun = None\n",
    "                    \n",
    "        if verb is None:\n",
    "            return None\n",
    "        \n",
    "        # build output string\n",
    "        joshis = sorted(joshis, key=lambda x: x[0])\n",
    "        joshi_chars = [c[0] for c in joshis]\n",
    "        joshi_chunks = [c[1] for c in joshis]\n",
    "        joshis_str = \" \".join(joshi_chars)\n",
    "        joshis_chunks_str = \" \".join(joshi_chunks)\n",
    "        ret_str = verb.find_verb()\n",
    "        ret_str = sahen_noun.surfaces + ret_str\n",
    "        if len(joshis_str) > 0:\n",
    "            ret_str += f\" {joshis_str} {joshis_chunks_str}\"\n",
    "                    \n",
    "        return ret_str\n",
    "    \n",
    "    def build_from_noun_to_root(self):\n",
    "        patterns = []\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if chunk.is_has_noun():\n",
    "                chunks_to_root = [chunk]\n",
    "                if chunk.dst == -1:\n",
    "                    patterns.append(chunk.surfaces)\n",
    "                    continue\n",
    "                next_chunk = self.chunks[chunk.dst]\n",
    "                while True:\n",
    "                    chunks_to_root.append(next_chunk)\n",
    "                    if next_chunk.dst == -1:\n",
    "                        # print(next_chunk.surfaces)\n",
    "                        break\n",
    "                    next_chunk = self.chunks[next_chunk.dst]\n",
    "                        \n",
    "                # build output\n",
    "                out_str = chunk.surfaces\n",
    "                for j, c in enumerate(chunks_to_root):\n",
    "                    if j == 0:\n",
    "                        continue\n",
    "                    out_str += f\" -> {c.surfaces}\"\n",
    "                patterns.append(out_str)\n",
    "                \n",
    "        return patterns\n",
    "    \n",
    "    def search_noun_relation_path(self):\n",
    "        patterns = []\n",
    "        for i, chunk in enumerate(self.shunks):\n",
    "            if chunk.is_has_noun():\n",
    "                pass\n",
    "                \n",
    "                \n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def analyze_cabocha_text(text_lines):\n",
    "    level = 0\n",
    "    chunks = []\n",
    "    morphs = []\n",
    "    for line in text_lines:\n",
    "        if line == 'EOS\\n':\n",
    "            if len(morphs) != 0:\n",
    "                chunks.append(Chunk(morphs=morphs, index=clause_num, dst=next_clause, srcs=None))\n",
    "                morphs = []\n",
    "                clause_num = None\n",
    "                next_clause = None\n",
    "            if len(chunks) != 0:\n",
    "                yield Sentence(chunks=Chunks(chunks))\n",
    "                chunks = []\n",
    "            continue\n",
    "        else:\n",
    "            if line[0] == '*':\n",
    "                if len(morphs) != 0:\n",
    "                    chunks.append(Chunk(morphs=morphs, index=clause_num, dst=next_clause, srcs=None))\n",
    "                    # if last character is \"。\" then it is end of sentence.\n",
    "                    if morphs[-1].pos1 == \"句点\":\n",
    "                        if len(chunks) != 0:\n",
    "                            yield Sentence(chunks=Chunks(chunks))\n",
    "                            chunks = []\n",
    "                    morphs = []\n",
    "                    clause_num = None\n",
    "                    next_clause = None\n",
    "                    \n",
    "                metas = line.split(' ')\n",
    "                clause_num = int(metas[1])\n",
    "                next_clause = int(metas[2].split('D')[0])\n",
    "                subj_num = int(metas[3].split('/')[0])\n",
    "                func_num = int(metas[3].split('/')[1])\n",
    "                score = float(metas[4])\n",
    "                # print(f\"metaline {line}\\n{clause_num}:{next_clause}:{subj_num}:{func_num}:{score}\")\n",
    "                continue\n",
    "                \n",
    "            cols = line.split('\\t')\n",
    "            res_cols = cols[1].split(',')\n",
    "            \n",
    "            morphs.append(Morph(\n",
    "                cols[0],\n",
    "                res_cols[6],\n",
    "                res_cols[0],\n",
    "                res_cols[1]\n",
    "            ))\n",
    "    return\n",
    "        \n",
    "\n",
    "def read_lines(path):\n",
    "    morphs = []\n",
    "    tmp_text_lines = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            tmp_text_lines.append(line)\n",
    "     \n",
    "    return tmp_text_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. 係り元と係り先の文節の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\t語\n",
      "（じんこうちのう、\t語\n",
      "AI\t〈エーアイ〉）とは\n",
      "〈エーアイ〉）とは\t語\n",
      "「『計算\t（）』という\n",
      "（）』という\t道具を\n",
      "概念と\t道具を\n",
      "『コンピュータ\t（）』という\n",
      "（）』という\t道具を\n",
      "道具を\t用いて\n",
      "用いて\t研究する\n",
      "『知能』を\t研究する\n",
      "研究する\t計算機科学\n",
      "計算機科学\t（）の\n",
      "（）の\t一分野」を\n",
      "一分野」を\t指す\n",
      "指す\t語\n"
     ]
    }
   ],
   "source": [
    "# 42\n",
    "for i, sentence in enumerate(analyze_cabocha_text(read_lines(CABOCHA_ANALYZED_PATH)), 1):\n",
    "    for j, chunk in enumerate(sentence.chunks):\n",
    "         if chunk.dst != -1:\n",
    "                print(f\"{chunk.surfaces}\\t{sentence.chunks[chunk.dst].surfaces}\")\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43\n",
    "sentences = []\n",
    "\n",
    "for i, sentence in enumerate(analyze_cabocha_text(read_lines(CABOCHA_ANALYZED_PATH)), 1):\n",
    "    sentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\t語\n",
      "（じんこうちのう、\t語\n",
      "AI\t〈エーアイ〉）とは\n",
      "〈エーアイ〉）とは\t語\n",
      "「『計算\t（）』という\n",
      "概念と\t道具を\n",
      "『コンピュータ\t（）』という\n",
      "道具を\t用いて\n",
      "『知能』を\t研究する\n",
      "研究する\t計算機科学\n",
      "計算機科学\t（）の\n",
      "一分野」を\t指す\n",
      "「言語の\t推論\n",
      "理解や\t推論\n",
      "推論\t問題解決などの\n",
      "問題解決などの\t知的行動を\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "技術」、または\t研究分野」とも\n",
      "「計算機\t（コンピュータ）による\n",
      "（コンピュータ）による\t情報処理システムの\n",
      "知的な\t情報処理システムの\n",
      "情報処理システムの\t実現に関する\n",
      "設計や\t実現に関する\n",
      "実現に関する\t研究分野」とも\n",
      "研究分野」とも\tされる\n",
      "『日本大百科全書(ニッポニカ)』の\t解説で\n",
      "解説で\t述べている\n",
      "情報工学者・通信工学者の\t佐藤理史は\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n",
      "人間の\t知的能力を\n",
      "知的能力を\t実現する\n",
      "コンピュータ上で\t実現する\n",
      "実現する\t技術・ソフトウェア・コンピュータシステム\n",
      "様々な\t技術・ソフトウェア・コンピュータシステム\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for j, chunk in enumerate(sentence.chunks):\n",
    "        if chunk.dst != -1 and chunk.is_has_noun() == True:\n",
    "            print(f\"{chunk.surfaces}\\t{sentence.chunks[chunk.dst].surfaces}\")\n",
    "            \n",
    "    if i == 4:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44. 係り受け木の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44\n",
    "import pydot\n",
    "\n",
    "def create_edge(sentence):\n",
    "    edge = []\n",
    "    for j, chunk in enumerate(sentence.chunks):\n",
    "        if chunk.dst != -1:\n",
    "            edge.append(((j, chunk.surfaces), (chunk.dst, sentence.chunks[chunk.dst].surfaces)))\n",
    "            \n",
    "    return edge\n",
    "\n",
    "def save_dag_img(edge, name):\n",
    "    if len(edge) > 0:\n",
    "        graph = pydot.graph_from_edges(edge, directed=True)\n",
    "        graph.write_png(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, '人工知能'), (17, '語')),\n",
       " ((1, '（じんこうちのう、'), (17, '語')),\n",
       " ((2, 'AI'), (3, '〈エーアイ〉）とは')),\n",
       " ((3, '〈エーアイ〉）とは'), (17, '語')),\n",
       " ((4, '「『計算'), (5, '（）』という')),\n",
       " ((5, '（）』という'), (9, '道具を')),\n",
       " ((6, '概念と'), (9, '道具を')),\n",
       " ((7, '『コンピュータ'), (8, '（）』という')),\n",
       " ((8, '（）』という'), (9, '道具を')),\n",
       " ((9, '道具を'), (10, '用いて')),\n",
       " ((10, '用いて'), (12, '研究する')),\n",
       " ((11, '『知能』を'), (12, '研究する')),\n",
       " ((12, '研究する'), (13, '計算機科学')),\n",
       " ((13, '計算機科学'), (14, '（）の')),\n",
       " ((14, '（）の'), (15, '一分野」を')),\n",
       " ((15, '一分野」を'), (16, '指す')),\n",
       " ((16, '指す'), (17, '語'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_edge(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = 2\n",
    "save_dag_img(create_edge(sentences[sentence_idx]), f\"../output/section5/graph_{sentence_idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "# 46. 動詞の格フレーム情報の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる を 記号処理を\n"
     ]
    }
   ],
   "source": [
    "# 45, 46\n",
    "print(sentences[7].find_kaku_pattern())\n",
    "kaku_patterns = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    kaku_pattern = sentence.find_kaku_pattern()\n",
    "    if kaku_pattern is not None:\n",
    "        kaku_patterns.append(kaku_pattern)\n",
    "    \n",
    "\n",
    "with open(\"../output/section5/kaku_pattern.txt\", \"w\") as fout:\n",
    "    fout.write(\"\\n\".join(kaku_patterns))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data/kaku_pattern.txt | uniq -c | sort -rn | head\n",
    "# yuki@MacBook-Pro-2 $ sort data/kaku_pattern.txt | uniq -c | sort -rn | grep 行う\n",
    "#    5 行う\n",
    "#    1 行う から が が で に に に に において は は\n",
    "#    1 行う まで を\n",
    "#    1 行う と に に は を をめぐって\n",
    "#    1 行う が て で で に を を\n",
    "#    1 行う が で を を\n",
    "\n",
    "# list(analyze_text([\"ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47. 機能動詞構文のマイニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sahen_noun_kaku_patterns = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    kaku_pattern = sentence.find_sahen_kaku_pattern()\n",
    "    if kaku_pattern is not None:\n",
    "        sahen_noun_kaku_patterns.append(kaku_pattern)\n",
    "    \n",
    "\n",
    "with open(\"../output/section5/sahen_norn_kaku_pattern.txt\", \"w\") as fout:\n",
    "    fout.write(\"\\n\".join(sahen_noun_kaku_patterns))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['知的行動を代わる に 人間に',\n",
       " '推論・判断をする',\n",
       " '記号処理を用いる',\n",
       " '注目を集める が 「サポートベクターマシン」が',\n",
       " '学習を行う に を 元に 経験を',\n",
       " '流行を超える',\n",
       " '学習を繰り返す',\n",
       " '統計的学習をする で に を を通して ACT-Rでは 元に 推論ルールを 生成規則を通して',\n",
       " '進化を見せる て において は 加えて 生成技術において （敵対的生成ネットワーク）は',\n",
       " 'コンテンツ生成を行う']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sahen_noun_kaku_patterns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. 名詞から根へのパスの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_to_roots = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    noun_to_roots += sentence.build_from_noun_to_root()\n",
    "\n",
    "with open(\"../output/section5/noun_to_roots.txt\", \"w\") as fout:\n",
    "    fout.write(\"\\n\".join(noun_to_roots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人工知能',\n",
       " '人工知能 -> 語',\n",
       " '（じんこうちのう、 -> 語',\n",
       " 'AI -> 〈エーアイ〉）とは -> 語',\n",
       " '〈エーアイ〉）とは -> 語',\n",
       " '「『計算 -> （）』という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> （）の -> 一分野」を -> 指す -> 語',\n",
       " '概念と -> 道具を -> 用いて -> 研究する -> 計算機科学 -> （）の -> 一分野」を -> 指す -> 語',\n",
       " '『コンピュータ -> （）』という -> 道具を -> 用いて -> 研究する -> 計算機科学 -> （）の -> 一分野」を -> 指す -> 語',\n",
       " '道具を -> 用いて -> 研究する -> 計算機科学 -> （）の -> 一分野」を -> 指す -> 語',\n",
       " '『知能』を -> 研究する -> 計算機科学 -> （）の -> 一分野」を -> 指す -> 語']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_to_roots[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 49. 名詞間の係り受けパスの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
